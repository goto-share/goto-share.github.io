{"pageProps":{"post":{"mdxSource":"var Component=(()=>{var c=Object.create;var r=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var o=e=>r(e,\"__esModule\",{value:!0});var m=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports),g=(e,n)=>{o(e);for(var s in n)r(e,s,{get:n[s],enumerable:!0})},b=(e,n,s)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let t of p(n))!f.call(e,t)&&t!==\"default\"&&r(e,t,{get:()=>n[t],enumerable:!(s=d(n,t))||s.enumerable});return e},u=e=>b(o(r(e!=null?c(h(e)):{},\"default\",e&&e.__esModule&&\"default\"in e?{get:()=>e.default,enumerable:!0}:{value:e,enumerable:!0})),e);var l=m((U,i)=>{i.exports=_jsx_runtime});var x={};g(x,{default:()=>y,frontmatter:()=>k});var a=u(l()),k={title:`Disaster recovery for multi-region Kafka at Uber | Uber Blog\n`,date:new Date(16085952e5),draft:!1,tags:[\"kafka\"],summary:`Uber has one of the largest deployments of Apache Kafka in the world, processing trillions of messages and multiple petabytes of data per day. As Figure 1 shows, today we position Apache Kafka as a cornerstone to Uber\\u2019s technology stack and build a complex ecosystem on top of it to empower a large number of different workflows. These include a pub/sub message bus to pass event data from the rider and driver apps, enabling a streaming analytics platform (e.g. Apache Samza, Apache Flink), streaming database changelogs to the downstream subscribers, and ingesting all sorts of data into Uber\\u2019s Apache Hadoop data lake.\n`};function w(e={}){let{wrapper:n}=e.components||{};return n?(0,a.jsx)(n,Object.assign({},e,{children:(0,a.jsx)(s,{})})):s();function s(){let t=Object.assign({h2:\"h2\",a:\"a\",span:\"span\",p:\"p\",ul:\"ul\",li:\"li\"},e.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(t.h2,{id:\"content\",children:[(0,a.jsx)(t.a,{href:\"#content\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,a.jsx)(t.span,{className:\"icon icon-link\"})}),\"Content\"]}),(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:\"https://eng.uber.com/kafka/\",children:\"https://eng.uber.com/kafka/\"})}),(0,a.jsxs)(t.h2,{id:\"link-preview\",children:[(0,a.jsx)(t.a,{href:\"#link-preview\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,a.jsx)(t.span,{className:\"icon icon-link\"})}),\"Link Preview\"]}),(0,a.jsxs)(t.ul,{children:[(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:\"https://eng.uber.com/kafka/\",children:\"https://eng.uber.com/kafka/\"})}),(0,a.jsx)(t.li,{children:\"Disaster recovery for multi-region Kafka at Uber | Uber Blog\"}),(0,a.jsx)(t.li,{children:\"Uber has one of the largest deployments of Apache Kafka in the world, processing trillions of messages and multiple petabytes of data per day. As Figure 1 shows, today we position Apache Kafka as a cornerstone to Uber\\u2019s technology stack and build a complex ecosystem on top of it to empower a large number of different workflows. These include a pub/sub message bus to pass event data from the rider and driver apps, enabling a streaming analytics platform (e.g. Apache Samza, Apache Flink), streaming database changelogs to the downstream subscribers, and ingesting all sorts of data into Uber\\u2019s Apache Hadoop data lake.\"})]})]})}}var y=w;return x;})();\n;return Component;","toc":[{"value":"Content","url":"#content","depth":2},{"value":"Link Preview","url":"#link-preview","depth":2}],"frontMatter":{"readingTime":{"text":"2 min read","minutes":1.28,"time":76800,"words":256},"slug":"2020-12-22-849","fileName":"2020-12-22-849.mdx","title":"Disaster recovery for multi-region Kafka at Uber | Uber Blog\n","date":"2020-12-22T00:00:00.000Z","draft":false,"tags":["kafka"],"summary":"Uber has one of the largest deployments of Apache Kafka in the world, processing trillions of messages and multiple petabytes of data per day. As Figure 1 shows, today we position Apache Kafka as a cornerstone to Uber’s technology stack and build a complex ecosystem on top of it to empower a large number of different workflows. These include a pub/sub message bus to pass event data from the rider and driver apps, enabling a streaming analytics platform (e.g. Apache Samza, Apache Flink), streaming database changelogs to the downstream subscribers, and ingesting all sorts of data into Uber’s Apache Hadoop data lake.\n"}},"authorDetails":[{"readingTime":{"text":"1 min read","minutes":0.36,"time":21600,"words":72},"slug":["default"],"fileName":"default.md","name":"Hao Chen","avatar":"/static/images/avatar.jpg","occupation":"MegaEase Inc. Founder","company":"MegaEase Inc.","email":"haoel@hotmail.com","twitter":"https://twitter.com/haoel","linkedin":"https://www.linkedin.com/in/haoel","github":"https://github.com/haoel","date":null}],"prev":{"title":"Apache Kafka: The Definitive Guide | Confluent\n","date":"2020-12-11T00:00:00.000Z","draft":false,"tags":["kafka"],"summary":"Learn how to take full advantage of Apache Kafka, understand how Kafka works and how it’s designed with this comprehensive book.\n","slug":"2020-12-11-848"},"next":{"title":"今年在Gopher中国的分享主题\n","date":"2020-12-26T00:00:00.000Z","draft":false,"tags":["coolshell","golang"],"summary":"今年在Gopher中国的分享主题，终于成文完毕，一共9篇（今年没多长时间写博客，年底补齐）。\n","slug":"2020-12-26-859"}},"__N_SSG":true}